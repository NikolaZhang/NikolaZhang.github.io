---
isOriginal: false
title: 命令行工具
date: 2021-05-24
tag:
  - 爬虫
  - scrapy
  - command
category: scrapy
description: 如何使用scrapy命令行工具
image: http://image.nikolazhang.top/wallhaven-nrwq11.jpg
sticky: false
timeline: true
article: true
star: false
---

[https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/commands.html](https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/commands.html)

## 命令行工具(Command line tools)

***0.10 新版功能***

Scrapy是通过 `scrapy` 命令行工具进行控制的。 这里我们称之为 “Scrapy tool” 以用来和子命令进行区分。 对于子命令，我们称为 “command” 或者 “Scrapy commands”。

## 使用 `scrapy` 工具[¶](https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/commands.html#id1)

您可以以无参数的方式启动Scrapy工具。该命令将会给出一些使用帮助以及可用的命令:

```bash
Scrapy X.Y - no active project

Usage:
  scrapy <command> [options] [args]

Available commands:
  crawl         Run a spider
  fetch         Fetch a URL using the Scrapy downloader
[...]
```

## 创建项目[¶](https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/commands.html#id2)

```bash
# 该命令将会在 myproject 目录中创建一个Scrapy项目。
scrapy startproject myproject
```

## 控制项目[¶](https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/commands.html#id3)

您可以在您的项目中使用 `scrapy` 工具来对其进行控制和管理。

```bash
# 创建一个新的spider:
scrapy genspider mydomain mydomain.com
```

有些Scrapy命令(比如 `[crawl](https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/commands.html#std:command-crawl)`)要求必须在Scrapy项目中运行。 您可以通过下边的 [commands reference](https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/commands.html#topics-commands-ref) 来了解哪些命令需要在项目中运行，哪些不用。

另外要注意，有些命令在项目里运行时的效果有些许区别。 以fetch命令为例，如果被爬取的url与某个特定spider相关联， 则该命令将会使用spider的动作(spider-overridden behaviours)。 (比如spider指定的 `user_agent`)。 该表现是有意而为之的。一般来说， `fetch` 命令就是用来测试检查spider是如何下载页面。

## 可用的工具命令(tool commands)[¶](https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/commands.html#tool-commands)

Scrapy提供了**两种类型**的命令。一种必须在Scrapy项目中运行(针对项目(Project-specific)的命令)，另外一种则不需要(全局命令)。全局命令在项目中运行时的表现可能会与在非项目中运行有些许差别(因为可能会使用项目的设定)。

全局命令:

- `[startproject](https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/commands.html#std:command-startproject)`
- `[settings](https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/commands.html#std:command-settings)`
- `[runspider](https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/commands.html#std:command-runspider)`
- `[shell](https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/commands.html#std:command-shell)`
- `[fetch](https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/commands.html#std:command-fetch)`
- `[view](https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/commands.html#std:command-view)`
- `[version](https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/commands.html#std:command-version)`

项目(Project-only)命令:

- `[crawl](https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/commands.html#std:command-crawl)`
- `[check](https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/commands.html#std:command-check)`
- `[list](https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/commands.html#std:command-list)`
- `[edit](https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/commands.html#std:command-edit)`
- `[parse](https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/commands.html#std:command-parse)`
- `[genspider](https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/commands.html#std:command-genspider)`
- `[deploy](https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/commands.html#std:command-deploy)`
- `[bench](https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/commands.html#std:command-bench)`

### 1. startproject[¶](https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/commands.html#startproject)

在 `project_name` 文件夹下创建一个名为 `project_name` 的Scrapy项目。

> `scrapy startproject <project_name>`

例子:

`scrapy startproject myproject`

### 2. genspider[¶](https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/commands.html#genspider)

在当前项目中创建spider。

> `scrapy genspider [-t template] <name> <domain>`

这仅仅是创建spider的一种快捷方法。该方法可以使用提前定义好的模板来生成spider。您也可以自己创建spider的源码文件。

例子:

```python
$ scrapy genspider -l
Available templates:
basic
crawl
csvfeed
xmlfeed

$ scrapy genspider -d basic
import scrapy
class $classname(scrapy.Spider):
	name = "$name"
	allowed_domains = ["$domain"]
	start_urls = (
	'[http://www.$domain/](http://www.%24domain/)',
	)
	def parse(self, response):
	    pass

$ scrapy genspider -t basic example [example.com](http://example.com/)
Created spider 'example' using template 'basic' in module:
mybot.spiders.exampl
```

### 3. crawl[¶](https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/commands.html#crawl)

使用spider进行爬取。

> `scrapy crawl <spider>`

例子:

```bash
$ scrapy crawl myspider
[ ... myspider starts crawling ... ]
```

### 4. check[¶](https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/commands.html#check)

运行contract检查。

> `scrapy check [-l] <spider>`

例子:

```bash
$ scrapy check -l
first_spider
  * parse
  * parse_item
second_spider
  * parse
  * parse_item

$ scrapy check
[FAILED] first_spider:parse_item
>>> 'RetailPricex' field is missing

[FAILED] first_spider:parse
>>> Returned 92 requests, expected 0..4
```

### 5. list[¶](https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/commands.html#list)

列出当前项目中所有可用的spider。每行输出一个spider。

> `scrapy list`

使用例子:

```bash
$ scrapy list
spider1
spider2
```

### 6. edit[¶](https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/commands.html#edit)

使用 `[EDITOR](https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/settings.html#std:setting-EDITOR)` 中设定的编辑器编辑给定的spider

> `scrapy edit <spider>`

该命令仅仅是提供一个快捷方式。开发者可以自由选择其他工具或者IDE来编写调试spider。

例子:

```bash
$ scrapy edit spider1
```

### 7. fetch[¶](https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/commands.html#fetch)

使用Scrapy下载器(downloader)下载给定的URL，并将获取到的内容送到标准输出。

> `scrapy fetch <url>`

该命令以spider下载页面的方式获取页面。例如，如果spider有 `USER_AGENT` 属性修改了 User Agent，该命令将会使用该属性。

因此，您可以使用该命令来查看spider如何获取某个特定页面。

该命令如果非项目中运行则会使用默认Scrapy downloader设定。

例子:

```bash
$ scrapy fetch --nolog http://www.example.com/some/page.html
[ ... html content here ... ]

$ scrapy fetch --nolog --headers http://www.example.com/
{'Accept-Ranges': ['bytes'],
 'Age': ['1263   '],
 'Connection': ['close     '],
 'Content-Length': ['596'],
 'Content-Type': ['text/html; charset=UTF-8'],
 'Date': ['Wed, 18 Aug 2010 23:59:46 GMT'],
 'Etag': ['"573c1-254-48c9c87349680"'],
 'Last-Modified': ['Fri, 30 Jul 2010 15:30:18 GMT'],
 'Server': ['Apache/2.2.3 (CentOS)']}
```

### 8. view[¶](https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/commands.html#view)

在浏览器中打开给定的URL，并以Scrapy spider获取到的形式展现。 有些时候spider获取到的页面和普通用户看到的并不相同。 因此该命令可以用来检查spider所获取到的页面，并确认这是您所期望的。

> `scrapy view <url>`

例子:

```bash
$ scrapy view http://www.example.com/some/page.html
[ ... browser starts ... ]
```

### 9. shell[¶](https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/commands.html#shell)

以给定的URL(如果给出)或者空(没有给出URL)启动Scrapy shell。 查看 [Scrapy终端(Scrapy shell)](https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/shell.html#topics-shell) 获取更多信息。

> `scrapy shell [url]`

例子:

```bash
$ scrapy shell http://www.example.com/some/page.html
[ ... scrapy shell starts ... ]
```

### 10. parse[¶](https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/commands.html#parse)

获取给定的URL并使用相应的spider分析处理。如果您提供 `--callback` 选项，则使用spider的该方法处理，否则使用 `parse` 。

> `scrapy parse <url> [options]`

支持的选项:

- `-spider=SPIDER`: 跳过自动检测spider并强制使用特定的spider
- `-a NAME=VALUE`: 设置spider的参数(可能被重复)
- `-callback` or `c`: spider中用于解析返回(response)的回调函数
- `-pipelines`: 在pipeline中处理item
- `-rules` or `r`: 使用 `[CrawlSpider](https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/spiders.html#scrapy.contrib.spiders.CrawlSpider)` 规则来发现用来解析返回(response)的回调函数
- `-noitems`: 不显示爬取到的item
- `-nolinks`: 不显示提取到的链接
- `-nocolour`: 避免使用pygments对输出着色
- `-depth` or `d`: 指定跟进链接请求的层次数(默认: 1)
- `-verbose` or `v`: 显示每个请求的详细信息

例子:

```bash
$ scrapy parse http://www.example.com/ -c parse_item
[ ... scrapy log lines crawling example.com spider ... ]

>>> STATUS DEPTH LEVEL 1 <<<
# Scraped Items  ------------------------------------------------------------
[{'name': u'Example item',
 'category': u'Furniture',
 'length': u'12 cm'}]

# Requests  -----------------------------------------------------------------
[]
```

### 11. settings[¶](https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/commands.html#settings)

获取Scrapy的设定, 在项目中运行时，该命令将会输出项目的设定值，否则输出Scrapy默认设定。

> `scrapy settings [options]`

例子:

```bash
$ scrapy settings --get BOT_NAME
scrapybot
$ scrapy settings --get DOWNLOAD_DELAY
0
```

### 12. runspider[¶](https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/commands.html#runspider)

在未创建项目的情况下，运行一个编写在Python文件中的spider。

> `scrapy runspider <spider_file.py>`

例子:

```bash
$ scrapy runspider myspider.py
[ ... spider starts crawling ... ]
```

### 13. version[¶](https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/commands.html#version)

输出Scrapy版本。配合 `-v` 运行时，该命令同时输出Python, Twisted以及平台的信息，方便bug提交。

> `scrapy version [-v]`

### 14. deploy[¶](https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/commands.html#deploy)

将项目部署到Scrapyd服务。查看 [部署您的项目](http://scrapyd.readthedocs.org/en/latest/deploy.html) 。

> `scrapy deploy [ <target:project> | -l <target> | -L ]`

### 15. bench[¶](https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/commands.html#bench)

运行benchmark测试。 [Benchmarking](https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/benchmarking.html#benchmarking) 。

> `scrapy bench`

## 自定义项目命令[¶](https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/commands.html#id4)

您也可以通过 `[COMMANDS_MODULE](https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/commands.html#std:setting-COMMANDS_MODULE)` 来添加您自己的项目命令。您可以以 [`scrapy/commands`](https://github.com/scrapy/scrapy/blob/master/scrapy/commands) 中Scrapy commands为例来了解如何实现您的命令。

### COMMANDS_MODULE[¶](https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/commands.html#commands-module)

用于查找添加自定义Scrapy命令的模块。

`Default: '' (empty string)`

例子:

```bash
COMMANDS_MODULE = 'mybot.commands'
```