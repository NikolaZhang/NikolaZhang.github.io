const e=JSON.parse('{"key":"v-7203a90d","path":"/posts/scrapy/Feed%20exports.html","title":"Feed exports","lang":"en-US","frontmatter":{"isOriginal":false,"title":"Feed exports","date":"2021-05-21T00:00:00.000Z","tag":["爬虫","scrapy","exports"],"category":"scrapy","description":"scrapy中如何Feed exports","image":"http://image.nikolazhang.top/wallhaven-nrwq11.jpg","sticky":false,"timeline":true,"article":true,"star":false,"head":[["meta",{"property":"og:url","content":"https://nikolazhang.github.io/posts/scrapy/Feed%20exports.html"}],["meta",{"property":"og:title","content":"Feed exports"}],["meta",{"property":"og:description","content":"scrapy中如何Feed exports"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"en-US"}],["meta",{"property":"og:updated_time","content":"2023-06-09T00:57:17.000Z"}],["meta",{"property":"article:author","content":"我小叮当、"}],["meta",{"property":"article:tag","content":"爬虫"}],["meta",{"property":"article:tag","content":"scrapy"}],["meta",{"property":"article:tag","content":"exports"}],["meta",{"property":"article:published_time","content":"2021-05-21T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2023-06-09T00:57:17.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"Feed exports\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2021-05-21T00:00:00.000Z\\",\\"dateModified\\":\\"2023-06-09T00:57:17.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"我小叮当、\\",\\"url\\":\\"https://nikolazhang.github.io\\"}]}"]]},"headers":[{"level":2,"title":"存储","slug":"存储","link":"#存储","children":[]},{"level":2,"title":"存储URI参数","slug":"存储uri参数","link":"#存储uri参数","children":[]}],"git":{"createdTime":1686272237000,"updatedTime":1686272237000,"contributors":[{"name":"dewy yr","email":"nikolazhang@163.com","commits":1}]},"readingTime":{"minutes":3.41,"words":1024},"filePathRelative":"posts/scrapy/Feed exports.md","localizedDate":"May 21, 2021","excerpt":"<blockquote>\\n<p>实现爬虫时最经常提到的需求就是能合适的保存爬取到的数据，或者说，生成一个带有爬取数据的”输出文件”(通常叫做”输出feed”)，来供其他系统使用。\\nScrapy自带了Feed输出，并且支持多种序列化格式(serialization format)及存储方式(storage backends)。</p>\\n</blockquote>\\n<p>可以通过 FEED_EXPORTERS 设置扩展支持的属性。</p>\\n<p><strong><strong>JSON</strong></strong>\\n• <strong><code>[FEED_FORMAT](https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/feed-exports.html#std:setting-FEED_FORMAT)</code></strong>:&nbsp;<code>json</code>\\n• 使用的exporter:&nbsp;<strong><code>[JsonItemExporter](https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/exporters.html#scrapy.contrib.exporter.JsonItemExporter)</code></strong>\\n• 大数据量情况下使用JSON请参见&nbsp;<a href=\\"https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/exporters.html#json-with-large-data\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">这个警告</a></p>"}');export{e as data};
