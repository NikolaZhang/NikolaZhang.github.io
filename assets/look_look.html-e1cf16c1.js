import{_ as d}from"./plugin-vue_export-helper-c27b6911.js";import{r as s,o as r,c as n,a as e,b as t,d as l,e as i}from"./app-2cb008ec.js";const o={},c=i('<h1 id="look-look" tabindex="-1"><a class="header-anchor" href="#look-look" aria-hidden="true">#</a> look_look</h1><blockquote><p>基于scrapy框架的爬虫项目，用于爬取常见网站的信息。<br> 不要遮遮掩掩的，让我康康！</p></blockquote><h2 id="环境配置" tabindex="-1"><a class="header-anchor" href="#环境配置" aria-hidden="true">#</a> 环境配置</h2><h3 id="创建conda环境" tabindex="-1"><a class="header-anchor" href="#创建conda环境" aria-hidden="true">#</a> 创建conda环境</h3><p>conda create -n look_look python=3.11.2</p><h3 id="安装依赖" tabindex="-1"><a class="header-anchor" href="#安装依赖" aria-hidden="true">#</a> 安装依赖</h3><p>conda install --yes --file requirements.txt</p><h2 id="目录结构" tabindex="-1"><a class="header-anchor" href="#目录结构" aria-hidden="true">#</a> 目录结构</h2><h3 id="python包" tabindex="-1"><a class="header-anchor" href="#python包" aria-hidden="true">#</a> python包：</h3>',9),h=i(`<li><p>config<br> 自定义的系统配置，比如驱动，文件路径，redis key。<br> 该包下的配置，可以在项目中直接引用，或者使用scrapy框架内的spider对象获取settings。<br> 如果想让后者生效，需要在项目中的<code>settings.py</code>中引入对应的python文件，如下：</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code> <span class="token keyword">from</span> config<span class="token punctuation">.</span>driver_config <span class="token keyword">import</span> <span class="token operator">*</span>
 <span class="token keyword">from</span> config<span class="token punctuation">.</span>web_config <span class="token keyword">import</span> <span class="token operator">*</span>
 <span class="token keyword">from</span> config<span class="token punctuation">.</span>path_config <span class="token keyword">import</span> <span class="token operator">*</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li><li><p>look_look/enhance<br> 通常用于定义一些接口，或者爬虫通用的实现方法。</p></li><li><p>look_look/spider<br> scrapy框架spider放到这个地方</p></li>`,3),p={href:"http://items.py",target:"_blank",rel:"noopener noreferrer"},g=e("br",null,null,-1),m={href:"http://middlewares.py",target:"_blank",rel:"noopener noreferrer"},k=e("br",null,null,-1),b={href:"http://pipelines.py",target:"_blank",rel:"noopener noreferrer"},f=e("br",null,null,-1),y=e("li",null,[e("p",null,[t("utils"),e("br"),t(" 工具方法")])],-1),x=i('<h3 id="pipelines" tabindex="-1"><a class="header-anchor" href="#pipelines" aria-hidden="true">#</a> pipelines</h3><table><thead><tr><th style="text-align:left;">class</th><th style="text-align:left;">description</th></tr></thead><tbody><tr><td style="text-align:left;">ExcelPipeline</td><td style="text-align:left;">将item信息保存到excel，支持header自定义</td></tr><tr><td style="text-align:left;">ImageSavePipeline</td><td style="text-align:left;">图片链接保存</td></tr><tr><td style="text-align:left;">MarkdownPipeline</td><td style="text-align:left;">将html转换为markdown</td></tr><tr><td style="text-align:left;">MongoDBPipeline</td><td style="text-align:left;">将item信息存储到mongo</td></tr><tr><td style="text-align:left;">RedisPipeline</td><td style="text-align:left;">将链接存放到redis</td></tr></tbody></table><h3 id="middlewares" tabindex="-1"><a class="header-anchor" href="#middlewares" aria-hidden="true">#</a> middlewares</h3><table><thead><tr><th style="text-align:left;">class</th><th style="text-align:left;">description</th></tr></thead><tbody><tr><td style="text-align:left;">ChromeDownloaderMiddleware</td><td style="text-align:left;">使用selenium获取动态网页内容</td></tr><tr><td style="text-align:left;">RandomUserAgentMiddleware</td><td style="text-align:left;">生成随机user-agent</td></tr><tr><td style="text-align:left;">CustomHeadersMiddleware</td><td style="text-align:left;">添加自定义的header，同时还具有缓存header的功能，不过似乎没什么用e_e。</td></tr></tbody></table><h3 id="enhance" tabindex="-1"><a class="header-anchor" href="#enhance" aria-hidden="true">#</a> enhance</h3><ol><li><p>base_item</p><table><thead><tr><th style="text-align:left;">class</th><th style="text-align:left;">description</th></tr></thead><tbody><tr><td style="text-align:left;">IItemSaveAction</td><td style="text-align:left;">提供保存item信息时需要提供的字段，下面的Action为具体化后的接口，配合指定的pipeline使用</td></tr><tr><td style="text-align:left;">RedisItemSaveAction</td><td style="text-align:left;">配合RedisPipeline使用</td></tr><tr><td style="text-align:left;">ExcelItemSaveAction</td><td style="text-align:left;">配合ExcelPipeline使用</td></tr><tr><td style="text-align:left;">MarkdownItemSaveAction</td><td style="text-align:left;">配合MarkdownPipeline使用</td></tr><tr><td style="text-align:left;">ImageItemSaveAction</td><td style="text-align:left;">配合ImageSavePipeline使用</td></tr></tbody></table></li><li><p>base_spider</p><table><thead><tr><th style="text-align:left;">class</th><th style="text-align:left;">method</th><th style="text-align:left;">description</th></tr></thead><tbody><tr><td style="text-align:left;">BaseSpider</td><td style="text-align:left;">get_run_config</td><td style="text-align:left;">加载运行配置:spider_config.RUN_SPIDER</td></tr><tr><td style="text-align:left;">RedisCategorySpider</td><td style="text-align:left;">get_category</td><td style="text-align:left;">存放redis时，指定key</td></tr><tr><td style="text-align:left;">RedisCategorySpider</td><td style="text-align:left;">get_key</td><td style="text-align:left;">存放redis时，生成key</td></tr><tr><td style="text-align:left;">SeleniumSpider</td><td style="text-align:left;">middleware_apply</td><td style="text-align:left;">使用selenium进行爬取</td></tr><tr><td style="text-align:left;">CustomHeaderSpider</td><td style="text-align:left;">add</td><td style="text-align:left;">添加header，比如下载pixiv图片需要referer</td></tr><tr><td style="text-align:left;">CustomHeaderSpider</td><td style="text-align:left;">get_custom_headers</td><td style="text-align:left;">获取自定义的header</td></tr><tr><td style="text-align:left;">BaseLoginSpider</td><td style="text-align:left;">login</td><td style="text-align:left;">提供登录接口，所有的登录都要实现该接口</td></tr><tr><td style="text-align:left;">PixivLoginSpider</td><td style="text-align:left;">login</td><td style="text-align:left;">pixiv的登录实现</td></tr></tbody></table></li><li><p>item_loaders</p><table><thead><tr><th style="text-align:left;">class</th><th style="text-align:left;">description</th></tr></thead><tbody><tr><td style="text-align:left;">CleanItemLoader</td><td style="text-align:left;">自定义的itemloader，用于清理item字段的空格</td></tr></tbody></table></li></ol><h3 id="其他" tabindex="-1"><a class="header-anchor" href="#其他" aria-hidden="true">#</a> 其他</h3><ol><li>logs 日志存放目录</li><li>data 爬取的文件存放目录</li></ol><blockquote><p>如果缺少这两个目录需要创建。或者在<code>settings.py</code>、以及上文说的<code>config</code>中另外指定</p></blockquote><h2 id="运行" tabindex="-1"><a class="header-anchor" href="#运行" aria-hidden="true">#</a> 运行</h2><p>运行项目只需要启动<code>main.py</code>即可。</p><h2 id="过程与结果" tabindex="-1"><a class="header-anchor" href="#过程与结果" aria-hidden="true">#</a> 过程与结果</h2><p>以csdn文章爬取为例：</p><ol><li>首先通过<code>CSDNSearchSpider</code>查询相关主题的文章链接，并保存到redis中。</li><li><code>CSDNArticleSpider</code>会监听redis中特定key下面是否有文章链接。有则进行文章的爬取。</li><li>提取文章的标题、作者、点赞量、收藏量、阅读量等信息存储到excel中；并将文章的内容转换为markdown格式，保存到data目录中。</li></ol><h3 id="文章链接" tabindex="-1"><a class="header-anchor" href="#文章链接" aria-hidden="true">#</a> 文章链接</h3><p><img src="https://raw.githubusercontent.com/NikolaZhang/image-blog/main/look_look/url.png" alt="文章链接"></p><h3 id="文章信息" tabindex="-1"><a class="header-anchor" href="#文章信息" aria-hidden="true">#</a> 文章信息</h3><p><img src="https://raw.githubusercontent.com/NikolaZhang/image-blog/main/look_look/article.png" alt="文章信息"></p><h3 id="文章内容" tabindex="-1"><a class="header-anchor" href="#文章内容" aria-hidden="true">#</a> 文章内容</h3><p><img src="https://raw.githubusercontent.com/NikolaZhang/image-blog/main/look_look/article_content.png" alt="文章内容"></p><h3 id="如何创建一个新的网站spider" tabindex="-1"><a class="header-anchor" href="#如何创建一个新的网站spider" aria-hidden="true">#</a> 如何创建一个新的网站spider?</h3><ol><li>首先在spiders包下，新增该网站的爬虫的py文件，名字最好是该网站的名字</li><li>接下来你需要观察网页的构造。一般情况下，分为两个层次，最外层是一个列表，每一项会跳转到对应的详情页。</li><li>通常列表页，需要创建一个spider，用于爬取这些详情页的链接，爬取到的链接需要放到redis中。</li><li>之后再创建一个spider，用于爬取详情页数据。这里的spider需要是RedisSpider的子类。并且指定redis_key。</li><li>如果我们希望添加一些pipeline的能力，需要设置spider的custom_settings属性</li><li>spider的parse方法中需要进行字段的提取，建议使用CleanItemLoader，它可以将字段首位的空格换行符号去除。</li></ol><p><strong>关于item的创建：</strong><br> 在第3步中，我们爬取链接并放到redis中，此时我们需要item具有提供url值的能力。只需要实现RedisItemSaveAction的get_category_field方法即可， 该方法需要返回字段名称，而不是字段值。</p><p>在base_item中还有其他的item父类，用于支持对应的pipeline能力。如：将item保存到mongo；导出item到excel；保存图片；保存网页为markdown等。</p><p>item一般放在items.py中，当然也可以创建新的文件存放。该项目之后都会放到spider中。（毕竟错误提交或者多提交了个人的spider相关的东西，被别人看到，不太好）</p><p>你可以参考项目中已经存在spider进行编程。</p><h2 id="调试" tabindex="-1"><a class="header-anchor" href="#调试" aria-hidden="true">#</a> 调试</h2><p>执行<code>main.py</code>可以使用debug模式。但是使用命令行则不会触发debug。<br> 如果程序报错，可以查看logs目录下的<code>look_look.log</code>文件中的报错信息。</p><h2 id="支持的网站及功能" tabindex="-1"><a class="header-anchor" href="#支持的网站及功能" aria-hidden="true">#</a> 支持的网站及功能</h2><h3 id="csdn" tabindex="-1"><a class="header-anchor" href="#csdn" aria-hidden="true">#</a> csdn</h3><ul class="task-list-container"><li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" id="task-item-0" checked="checked" disabled="disabled"><label class="task-list-item-label" for="task-item-0"> 查询页博客链接提取（存储到redis）</label></li><li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" id="task-item-1" checked="checked" disabled="disabled"><label class="task-list-item-label" for="task-item-1"> 根据博客链接爬取文章（保存excel，mongo，markdown）</label></li></ul><h4 id="效果" tabindex="-1"><a class="header-anchor" href="#效果" aria-hidden="true">#</a> 效果</h4><p>见上【过程与结果】</p><h3 id="pixiv" tabindex="-1"><a class="header-anchor" href="#pixiv" aria-hidden="true">#</a> pixiv</h3><ul class="task-list-container"><li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" id="task-item-2" checked="checked" disabled="disabled"><label class="task-list-item-label" for="task-item-2"> 获取用户下的作品集</label></li><li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" id="task-item-3" checked="checked" disabled="disabled"><label class="task-list-item-label" for="task-item-3"> 作品及详情信息获取</label></li><li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" id="task-item-4" checked="checked" disabled="disabled"><label class="task-list-item-label" for="task-item-4"> 图片下载</label></li></ul><h4 id="效果-1" tabindex="-1"><a class="header-anchor" href="#效果-1" aria-hidden="true">#</a> 效果</h4><p>mongodb中存储信息<br><img src="https://raw.githubusercontent.com/NikolaZhang/image-blog/main/look_look/art_work1.png" alt="art1"> 导出到excel中的信息<br><img src="https://raw.githubusercontent.com/NikolaZhang/image-blog/main/look_look/art_work2.png" alt="art1"> 保存的图片<br><img src="https://raw.githubusercontent.com/NikolaZhang/image-blog/main/look_look/art_work3.png" alt="art1"></p><h2 id="q-a" tabindex="-1"><a class="header-anchor" href="#q-a" aria-hidden="true">#</a> Q&amp;A</h2><ol><li>如何指定启动哪些spider?<br> 在main.py中我们通过代码的方式启动爬虫，因此可以设置你期望的爬虫类。</li><li>如何定制spider的middleware和pipeline?<br> 这个属于scrapy的基本功能，可以在爬虫类中设置custom_settings属性。</li><li>动态网页如何爬取?<br> 该项目中引入了selenium依赖，需要你在middleware中对请求和响应通过selenium进行处理。可以参考ChromeDownloaderMiddleware</li><li>为什么redis中没有数据?<br> 首先查看日志中是否有报错日志。如果没有，需要检查xpath是否正确。在页面上f12，通过$x即可测试。</li><li>为什么爬虫直接退出了，且没有报错?<br> 如果是从redis拉取数据进行爬取，查看是否继承了RedisSpider或RedisCategorySpider。RedisSpider需要指定redis_key参数，RedisCategorySpider提供了一个从公共枚举和配置中获取该参数的方法，因此需要你指定枚举UrlType以及在配置文件（spider_config）中设置参数配置，该参数应该和之前的链接提取spider参数相同。</li><li>为什么浏览器一直白屏，后台不报错，似乎爬虫停住了?<br> 检查下redis中是否有数据，否则爬虫会等待redis中有值后才会继续。</li></ol><h2 id="获取" tabindex="-1"><a class="header-anchor" href="#获取" aria-hidden="true">#</a> 获取</h2>',40),u={href:"https://gitee.com/NikolaZhang/look_look",target:"_blank",rel:"noopener noreferrer"};function _(w,S){const a=s("ExternalLinkIcon");return r(),n("div",null,[c,e("ol",null,[h,e("li",null,[e("p",null,[e("a",p,[t("items.py"),l(a)]),g,t(" 用于定义一些页面的字段")])]),e("li",null,[e("p",null,[e("a",m,[t("middlewares.py"),l(a)]),k,t(" 用于处理请求和响应")])]),e("li",null,[e("p",null,[e("a",b,[t("pipelines.py"),l(a)]),f,t(" 用于处理item")])]),y]),x,e("p",null,[t("项目源码地址："),e("a",u,[t("https://gitee.com/NikolaZhang/look_look"),l(a)])])])}const N=d(o,[["render",_],["__file","look_look.html.vue"]]);export{N as default};
